{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import django\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from django.conf import settings\n",
    "from django.db import connection\n",
    "from hawc.apps.vocab.models import GuidelineProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export spreadsheet with corresponding HAWC vocab ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT\n",
    "    endpoint_target.id,\n",
    "    endpoint_category.name AS endpoint_category,\n",
    "\tendpoint_type.name AS endpoint_type,\n",
    "    endpoint_target.name AS endpoint_target\n",
    "FROM\n",
    "    public.vocab_term AS endpoint_target\n",
    "LEFT JOIN\n",
    "    public.vocab_term AS endpoint_type ON endpoint_target.parent_id = endpoint_type.id\n",
    "LEFT JOIN\n",
    "    public.vocab_term AS endpoint_category ON endpoint_type.parent_id = endpoint_category.id\n",
    "WHERE\n",
    "    endpoint_target.type = 4 AND endpoint_target.namespace=2;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdjango\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m settings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Combine the spreadsheet and replace the endpoint_ids with the Hawc toxref\n",
    "hawc_df = pd.read_csv('C:/Users/63080/Downloads/hawc/hawc_toxref_export.csv')\n",
    "guideline_df = pd.read_csv('C:/Users/63080/Downloads/hawc/guideline_profile.csv')\n",
    "\n",
    "for df in [hawc_df, guideline_df]:\n",
    "    df[\"endpoint_category\"] = df[\"endpoint_category\"].str.strip()\n",
    "    df[\"endpoint_type\"] = df[\"endpoint_type\"].str.strip()\n",
    "    df[\"endpoint_target\"] = df[\"endpoint_target\"].str.strip()\n",
    "\n",
    "# Merge the DataFrames on vocab columns\n",
    "df = pd.merge(guideline_df, hawc_df[['id', 'endpoint_category', 'endpoint_type', 'endpoint_target']],\n",
    "                     on=['endpoint_type', 'endpoint_category', 'endpoint_target'], how='left')\n",
    "\n",
    "# Replace the endpoint_id column values\n",
    "df['endpoint_id'] = df['id']\n",
    "\n",
    "# Drop the term columns as they're no longer needed\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.drop(columns=['endpoint_type'], inplace=True)\n",
    "df.drop(columns=['endpoint_category'], inplace=True)\n",
    "df.drop(columns=['endpoint_target'], inplace=True)\n",
    "df.rename(columns={'guideline_profile_id': 'id'}, inplace=True)\n",
    "\n",
    "# Clean up data\n",
    "df[\"obs_status\"] = df[\"obs_status\"].str.strip()\n",
    "df[\"description\"] = df[\"description\"].str.strip()\n",
    "\n",
    "# # Save the updated DataFrame back to a CSV file\n",
    "df.to_csv('guideline_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CSV file and create guideline objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new GuidelineProfile Objects\n",
    "GuidelineProfile.objects.all().delete()\n",
    "\n",
    "with open('guideline_updated.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        print(\"endpoint\", row['endpoint_id'])\n",
    "        # Add more fields as needed\n",
    "\n",
    "        row[\"endpoint_id\"] = int(row[\"endpoint_id\"])\n",
    "        # Create a new instance\n",
    "        GuidelineProfile.objects.create(**row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data fixture in JSONL instead of CSV\n",
    "file = settings.PROJECT_PATH / \"apps/vocab/fixtures/guideline.jsonl\"\n",
    "\n",
    "def _get_headers(cursor) -> list[str]:\n",
    "    cursor.execute(\"Select * FROM vocab_guidelineprofile LIMIT 0\")\n",
    "    return [desc[0] for desc in cursor.description]\n",
    "\n",
    "\n",
    "with connection.client.connection.cursor() as cursor:\n",
    "    headers = _get_headers(cursor)\n",
    "    cursor.execute(\"SELECT * FROM vocab_guidelineprofile\")\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    jsonl_data = []\n",
    "\n",
    "    for row in data:\n",
    "        row_dict = {header: row[i] for i, header in enumerate(headers)}\n",
    "        pk = row_dict[\"id\"]\n",
    "\n",
    "        fields = {}\n",
    "        for i, header in enumerate(headers):\n",
    "            # id should not be in fields\n",
    "            if header != \"id\":\n",
    "                fields[header] = row[i]\n",
    "\n",
    "        # JSONL\n",
    "        json_obj = {\"model\": \"vocab.GuidelineProfile\", \"pk\": pk, \"fields\": fields}\n",
    "        jsonl_data.append(json_obj)\n",
    "\n",
    "\n",
    "with open(file, \"w\") as jsonl_file:\n",
    "    for json_obj in jsonl_data:\n",
    "        jsonl_file.write(json.dumps(json_obj) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
