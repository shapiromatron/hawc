{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrate data from ToxRefDB to HAWC\n",
    "\n",
    "Resource: User Guide (https://nepis.epa.gov/Exe/ZyPDF.cgi/P1015KWT.PDF?Dockey=P1015KWT.PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import django\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "from asgiref.sync import sync_to_async  # access the ORM safely within the notebook\n",
    "from django.db.models import Q\n",
    "from django.urls import reverse\n",
    "from psycopg2.extras import DictCursor\n",
    "from rapidfuzz import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update environment\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"hawc.main.settings.local\")\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hawc.apps.animalv2 import constants, models\n",
    "from hawc.apps.assessment.models import DoseUnits, Species, Strain\n",
    "from hawc.apps.study.models import Study\n",
    "from hawc.apps.vocab.models import Guideline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import test studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hawc_client import HawcClient\n",
    "# client = HawcClient(\"http://127.0.0.1:8000\")\n",
    "# client.authenticate(\"pm@hawcproject.org\", \"pw\")\n",
    "\n",
    "# hero_ids = [11805638, 11805639, 11805640, 11347133, 11347134,\n",
    "#             11805642, 11347163, 11347164, 11347166, 11347167,\n",
    "#             11805643, 11347168, 11805644, 11347169, 11347170,\n",
    "#             11805645, 11347249, 11347258, 11816654, 11816658,\n",
    "#             12330123, 12329953, 5932278, 12329803, 12329975, 12000793]\n",
    "\n",
    "# for id in hero_ids:\n",
    "#     response = client.study.create_from_identifier(\n",
    "#         db_type=\"HERO\", db_id=id, assessment_id=1\n",
    "#     )\n",
    "#     print(\"repsonse: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up API access\n",
    "https://hawc.readthedocs.io/latest/client/#api-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login\n",
    "session = requests.Session()\n",
    "\n",
    "# base_url = https://hawcproject.org\n",
    "base_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "login = requests.post(\n",
    "    f\"{base_url}/user/api/token-auth/\",\n",
    "    json={\"username\": \"admin@hawcproject.org\", \"password\": \"pw\"},\n",
    "    timeout=10,\n",
    ")\n",
    "print(\"login status:\", login.json())\n",
    "session.headers.update(Authorization=f\"Token {login.json()['token']}\")\n",
    "login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define urls\n",
    "get_study_url = reverse(\"study:api:study-list\")\n",
    "print(\"get_study_url:\", get_study_url)\n",
    "# animalv2\n",
    "animalgroup_url = reverse(\"animalv2:api:animal-group-list\")\n",
    "chem_url = reverse(\"animalv2:api:chemical-list\")\n",
    "data_extraction_url = reverse(\"animalv2:api:data-extraction-list\")\n",
    "dose_group_url = reverse(\"animalv2:api:dose-group-list\")\n",
    "dose_response_group_level_data_url = reverse(\"animalv2:api:dose-response-group-level-data-list\")\n",
    "endpoint_url = reverse(\"animalv2:api:endpoint-list\")\n",
    "experiment_url = reverse(\"animalv2:api:experiment-list\")\n",
    "observation_time_url = reverse(\"animalv2:api:observation-time-list\")\n",
    "treatment_url = reverse(\"animalv2:api:treatment-list\")\n",
    "\n",
    "# vocab\n",
    "toxrefdb_vocab_url = reverse(\"vocab:api:toxrefdb-nested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up database functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db(db_name, user, password, host, port):\n",
    "    conn = psycopg2.connect(dbname=db_name, user=user, password=password, host=host, port=port)\n",
    "    print(f\"Connected to {db_name} successfully!\")\n",
    "    return conn\n",
    "\n",
    "\n",
    "def fetch_data_from_source(query, conn):\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall()\n",
    "\n",
    "\n",
    "def create_model_objects(url, data_list):\n",
    "    responses = []\n",
    "    for data in data_list:\n",
    "        response = session.post(f\"{base_url}/{url}\", data)\n",
    "        responses.append(response)\n",
    "        if response.status_code != 201:\n",
    "            print(\"status failed: \", response.json())\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "def get_model_objects(url):\n",
    "    response = session.get(f\"{base_url}/{url}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "# set to toxref db\n",
    "source_conn = connect_to_db(\"toxref\", \"hawc\", \"\", \"localhost\", \"5432\")\n",
    "cur = source_conn.cursor(cursor_factory=DictCursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ToxRefDB Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study types are found in the User Guide (https://nepis.epa.gov/Exe/ZyPDF.cgi/P1015KWT.PDF?Dockey=P1015KWT.PDF)\n",
    "# Data is split by study to avoid reading too much into memory at once.\n",
    "# (The official downloadable ToxRefDB seems to be 66.5 MB, so this shouldn't be an issue.)\n",
    "STUDY_TYPES = [\"CHR\", \"SUB\", \"SAC\", \"DEV\", \"MGR\", \"REP\", \"DNT\", \"ACU\", \"OTH\"]\n",
    "\n",
    "# Define query for study dose-response data\n",
    "## left join to include studies without tg or dose info\n",
    "query = \"\"\"SELECT * FROM prod_toxrefdb_2_1.chemical\n",
    "LEFT JOIN prod_toxrefdb_2_1.study ON study.chemical_id=chemical.chemical_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.tg ON tg.study_id=study.study_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.dose ON dose.study_id=study.study_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.dtg ON dtg.tg_id=tg.tg_id AND dose.dose_id=dtg.dose_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.tg_effect ON tg.tg_id=tg_effect.tg_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.effect ON effect.effect_id=tg_effect.effect_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.endpoint ON endpoint.endpoint_id=effect.endpoint_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.dtg_effect ON tg_effect.tg_effect_id=dtg_effect.tg_effect_id AND dtg.dtg_id=dtg_effect.dtg_id\n",
    "WHERE study.study_type = %s\"\"\"\n",
    "\n",
    "chem_study_query = \"\"\"SELECT * FROM prod_toxrefdb_2_1.chemical\n",
    "LEFT JOIN prod_toxrefdb_2_1.study ON study.chemical_id=chemical.chemical_id\n",
    "LEFT JOIN prod_toxrefdb_2_1.dose ON dose.study_id=study.study_id\n",
    "WHERE study.study_type = %s;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment and Chemical objects\n",
    "Uses info from ToxRefDB study and chemical tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsstox_array = []\n",
    "dsstox_ids = []\n",
    "\n",
    "# Read in all chemical dsstox_substance_ids\n",
    "for type in STUDY_TYPES:\n",
    "    cur.execute(chem_study_query, (type,))\n",
    "    rows = cur.fetchall()\n",
    "    dsstox = set(row[\"dsstox_substance_id\"] for row in rows)\n",
    "    dsstox_ids.extend(dsstox)\n",
    "\n",
    "# Create new DSSTox objects if missing\n",
    "dsstox_array.extend([{\"dtxsid\": value} for value in dsstox_ids])\n",
    "\n",
    "print(\"### creating dsstox items ###\")\n",
    "responses = create_model_objects(reverse(\"assessment:api:dsstox-list\"), dsstox_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map ToxRef design field values to HAWC constants\n",
    "experiment_type_map = {\n",
    "    \"CHR\": constants.ExperimentDesign.CH,\n",
    "    \"SUB\": constants.ExperimentDesign.SB,\n",
    "    \"DEV\": constants.ExperimentDesign.DV,\n",
    "    \"MGR\": constants.ExperimentDesign.R2,  #  2-generation reproductive\n",
    "    \"REP\": constants.ExperimentDesign.RP,\n",
    "    \"DNT\": constants.ExperimentDesign.OT,  #  other\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy study matching\n",
    "hawc_studies = await sync_to_async(Study.objects.values_list)(\"id\", \"full_citation\")\n",
    "studies_list = await sync_to_async(list)(hawc_studies)\n",
    "study_cit_map = [\n",
    "    (id, \"\".join(char for char in cit if char not in string.punctuation))\n",
    "    for id, cit in studies_list\n",
    "]\n",
    "\n",
    "\n",
    "async def get_hawc_study(trimmed_citation):\n",
    "    # Try finding the study by title\n",
    "    # Toxref format: J.C. Pettersen, A.D. Richter, and P.A. Gilles (1991) 90-Day Oral Toxicity Study in Rats. CIBA-GEIGY...\n",
    "    # Title: 90-Day Oral Toxicity Study in Rats.\n",
    "    title = re.search(r\"\\)\\.?(.*?)([:.])\", trimmed_citation)\n",
    "    study = None\n",
    "    if title and len(title.group(1)) > 25:\n",
    "        title = title.group(1).strip()\n",
    "        study = await sync_to_async(Study.objects.filter)(Q(full_citation__icontains=title))\n",
    "        study = await sync_to_async(study.first)()\n",
    "        print(\"study: \", study.full_citation if study else \"None found by title\")\n",
    "\n",
    "    # If you can't access by title (format is not always consistent) try fuzzy match\n",
    "    if not study:\n",
    "        # remove punctuation and anything after \"unpublished\"\n",
    "        trimmed_citation = \"\".join(\n",
    "            char for char in trimmed_citation if char not in string.punctuation\n",
    "        )\n",
    "        trimmed_citation = (\n",
    "            re.split(r\"unpublished\", trimmed_citation, flags=re.IGNORECASE)[0].strip().lower()\n",
    "        )\n",
    "        match, score, match_id = process.extractOne(\n",
    "            trimmed_citation,\n",
    "            [m[1].lower() for m in study_cit_map],\n",
    "            scorer=lambda a, b, **kwargs: fuzz.ratio(a[:50], b[:50]),\n",
    "        )\n",
    "        # print(\"toxref citation: \", trimmed_citation)\n",
    "        # print(\"match: \", match, \" score: \", score, \" match_id: \", match_id)\n",
    "        if score >= 90:\n",
    "            hawc_id = study_cit_map[match_id][0]\n",
    "            study = await sync_to_async(Study.objects.get)(id=hawc_id)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Create a new HAWC Experiment and Chemical for each ToxRef study item ###\")\n",
    "\n",
    "study_to_exp_map = {}\n",
    "toxref_to_hawc_chem_map = {}\n",
    "\n",
    "for type in STUDY_TYPES:\n",
    "    cur.execute(chem_study_query, (type,))\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Read into a df for data manipulation\n",
    "    column_names = [desc[0] for desc in cur.description]\n",
    "    df = pd.DataFrame(rows, columns=column_names)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    unique_studies = df[\"study_id\"].unique()\n",
    "\n",
    "    for study_id in unique_studies:\n",
    "        # get the first row for each study\n",
    "        row = df[df[\"study_id\"] == study_id].iloc[0]\n",
    "\n",
    "        # trimmed_citation = re.split(r'unpublished', toxref_citation, flags=re.IGNORECASE)[0].strip()\n",
    "        # ToxRefDB study_citation does not map to 1-1 with HAWC Study full_citation. Get just the study title\n",
    "        study = await get_hawc_study(row[\"study_citation\"].strip())\n",
    "        if not study:\n",
    "            print(\"WARNING: No matching study found for citation:\", row[\"study_citation\"].strip())\n",
    "            study = None\n",
    "\n",
    "        guideline = await sync_to_async(Guideline.objects.filter)(\n",
    "            Q(name__iexact=row[\"study_type_guideline\"]) | Q(id=row[\"guideline_id\"])\n",
    "        )\n",
    "        guideline = await sync_to_async(guideline.first)()\n",
    "\n",
    "        # Create a new experiment object\n",
    "        experiment_data = {\n",
    "            \"study\": study.id if study else 1,\n",
    "            \"name\": f\"{row['dose_end']} {row['dose_end_unit']} {row['admin_route']} {row['study_type']} {row['preferred_name']}\",  # Ex: 30 day oral CHR Isazofos\n",
    "            \"design\": experiment_type_map[row[\"study_type\"].upper()],\n",
    "            \"has_multiple_generations\": True if row[\"study_type\"] == \"MGR\" else False,\n",
    "            \"guideline\": guideline.id if guideline else None,\n",
    "            \"comments\": row[\"study_comment\"] or \"\",\n",
    "        }\n",
    "        experiment = create_model_objects(experiment_url, [experiment_data])[0].json()\n",
    "        # map ToxRefDB study to new  experiment\n",
    "        # print(\"experiment: \", experiment)\n",
    "        study_to_exp_map[row[\"study_id\"]] = experiment.get(\"id\")\n",
    "\n",
    "        # Create a new HAWC Chemical for each ToxRef study (many-to-one study->chemical to one-to-one study->chemical)\n",
    "        vehicle = row[\"vehicle\"]\n",
    "        if vehicle is None:\n",
    "            inhalation_study = row[\"admin_route\"].lower() == \"inhalation\"\n",
    "            vehicle = \"not reported, assumed clean air.\" if inhalation_study else \"not reported\"\n",
    "\n",
    "        chemical_data = {\n",
    "            \"name\": row[\"preferred_name\"],\n",
    "            \"dtxsid_id\": row[\"dsstox_substance_id\"],\n",
    "            \"cas\": row[\"casrn\"],\n",
    "            \"experiment\": experiment.get(\"id\"),\n",
    "            \"source\": row[\"substance_source_name\"] or \"\",\n",
    "            \"purity\": row[\"substance_purity\"] or \"\",\n",
    "            \"comments\": row[\"substance_comment\"] or \"\",\n",
    "            \"vehicle\": vehicle,\n",
    "        }\n",
    "        chemical = create_model_objects(chem_url, [chemical_data])[0].json()\n",
    "        # print(\"chemical: \", chemical)\n",
    "        toxref_to_hawc_chem_map[row[\"chemical_id\"]] = chemical.get(\"id\")\n",
    "\n",
    "print(study_to_exp_map)\n",
    "print(toxref_to_hawc_chem_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HAWC Animalgroup objects\n",
    "\n",
    "Uses info from ToxRefDB tg and study tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map ToxRef field values to HAWC constants\n",
    "life_stage_map = {\n",
    "    \"adult-pregnancy\": constants.Lifestage.AG,\n",
    "    \"fetal\": constants.Lifestage.DEV,  # verify\n",
    "    \"adult\": constants.Lifestage.ADULT,\n",
    "    \"juvenile\": constants.Lifestage.JUV,\n",
    "    None: \"\",\n",
    "}\n",
    "\n",
    "route_exposure_map = {\n",
    "    \"oral\": constants.RouteExposure.OR,\n",
    "    \"inhalation\": constants.RouteExposure.I,\n",
    "    \"dermal\": constants.RouteExposure.D,\n",
    "}\n",
    "\n",
    "variance_type_map = {\n",
    "    \"NA\": constants.VarianceType.NA.label,\n",
    "    \"NR\": constants.VarianceType.NR.label,\n",
    "    \"SD\": constants.VarianceType.SD.label,\n",
    "    \"SE\": constants.VarianceType.SE.label,\n",
    "    \"\": constants.VarianceType.SD.label,\n",
    "}\n",
    "\n",
    "observation_time_map = {\n",
    "    \"week\": constants.ObservationTimeUnits.WK.label,\n",
    "    \"lactation week\": constants.ObservationTimeUnits.WK.label,\n",
    "    \"month\": constants.ObservationTimeUnits.MON.label,\n",
    "    \"day\": constants.ObservationTimeUnits.DAY.label,\n",
    "    \"LD\": constants.ObservationTimeUnits.DAY.label,  # TODO: day vs lactation day?\n",
    "    \"GD\": constants.ObservationTimeUnits.GD.label,\n",
    "    \"PND\": constants.ObservationTimeUnits.PND.label,\n",
    "}\n",
    "\n",
    "treatment_related_map = {\n",
    "    \"true\": constants.TreatmentRelatedEffect.YES.label,\n",
    "    \"false\": constants.TreatmentRelatedEffect.NO.label,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new HAWC treatment, animalgroup, species, and strain if needed for each ToxRef tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxref_to_hawc_tg_map = {}\n",
    "# toxref will have duplicate treatment groups (with differences that are irrelevant in hawc)\n",
    "# we still need to identify them in order to create effects/etc. later\n",
    "duplicate_tg_ids = defaultdict(list)\n",
    "\n",
    "\n",
    "for type in STUDY_TYPES:\n",
    "    cur.execute(query, (type,))\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Read into a df for data manipulation\n",
    "    column_names = [desc[0] for desc in cur.description]\n",
    "    df = pd.DataFrame(rows, columns=column_names)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # avoid duplicate treatment groups by using all columns except 'dose_period'\n",
    "    tg_cols = [\n",
    "        \"study_id\",\n",
    "        \"sex\",\n",
    "        \"generation\",\n",
    "        \"dose_duration\",\n",
    "        \"dose_duration_unit\",\n",
    "        \"n\",\n",
    "        \"tg_comment\",\n",
    "    ]\n",
    "    tg_df = df[[\"tg_id\", *tg_cols]]\n",
    "\n",
    "    # store duplicate treatment groups\n",
    "    for _, group in tg_df.groupby(\n",
    "        [\"study_id\", \"dose_duration\", \"dose_duration_unit\", \"n\", \"tg_comment\"], dropna=False\n",
    "    ):\n",
    "        tg_ids = group[\"tg_id\"].tolist()\n",
    "        if len(tg_ids) >= 1:\n",
    "            duplicate_tg_ids[tuple(tg_ids)] = tg_ids\n",
    "\n",
    "    # filter unique treatment groups\n",
    "    unique_tg_df = tg_df.drop([\"sex\", \"generation\"], axis=1)\n",
    "    unique_tg_df = unique_tg_df.drop_duplicates(\n",
    "        subset=[col for col in unique_tg_df.columns if col != \"tg_id\"]\n",
    "    ).reset_index(drop=True)\n",
    "    unique_tgs = unique_tg_df[\"tg_id\"].unique()\n",
    "\n",
    "    for tg_id in unique_tgs:\n",
    "        row = df[df[\"tg_id\"] == tg_id].iloc[0]\n",
    "        experiment_id = study_to_exp_map[row[\"study_id\"]]\n",
    "\n",
    "        dose_duration = row[\"dose_duration\"]\n",
    "        if pd.isna(row[\"dose_duration\"]):\n",
    "            dose_duration = \"\"\n",
    "\n",
    "        exposure_duration_description = f\"{dose_duration} {row['dose_duration_unit']}\"\n",
    "        treatment_data = {\n",
    "            \"experiment\": experiment_id,\n",
    "            \"name\": f\"{exposure_duration_description} {row['admin_route']} {row['preferred_name']}\",\n",
    "            \"chemical_id\": toxref_to_hawc_chem_map[row[\"chemical_id\"]],\n",
    "            \"route_of_exposure\": route_exposure_map[row[\"admin_route\"].lower()],\n",
    "            \"exposure_duration\": dose_duration,\n",
    "            \"exposure_duration_description\": exposure_duration_description,\n",
    "            \"comments\": row[\"tg_comment\"] or \"\",\n",
    "        }\n",
    "        treatment = create_model_objects(treatment_url, [treatment_data])\n",
    "\n",
    "        # add all treatment ids to map\n",
    "        for tg_id_tuple in duplicate_tg_ids.keys():\n",
    "            if tg_id in tg_id_tuple:\n",
    "                for id in tg_id_tuple:\n",
    "                    toxref_to_hawc_tg_map[id] = treatment[0].json().get(\"id\")\n",
    "\n",
    "    # create animal groups for each treatment group\n",
    "    unique_tg_effects = tg_df.drop_duplicates(\n",
    "        subset=[col for col in tg_df.columns if col != \"tg_id\"]\n",
    "    ).reset_index(drop=True)\n",
    "    unique_tgs = unique_tg_effects[\"tg_id\"].unique()\n",
    "\n",
    "    for tg_id in unique_tgs:\n",
    "        row = df[df[\"tg_id\"] == tg_id].iloc[0]\n",
    "        experiment_id = study_to_exp_map[row[\"study_id\"]]\n",
    "\n",
    "        # create new animalgroup using tg and tg_effect\n",
    "        if pd.notna(row[\"tg_effect_id\"]):\n",
    "            # create new species and strain if necessary\n",
    "            species, created = await sync_to_async(Species.objects.get_or_create)(\n",
    "                name=row[\"species\"]\n",
    "            )\n",
    "            strain, created = await sync_to_async(Strain.objects.get_or_create)(\n",
    "                name=row[\"strain\"], species_id=species.id\n",
    "            )\n",
    "\n",
    "            if len(row[\"generation\"]) > 2:\n",
    "                gen = constants.Generation.F1  # default generation?\n",
    "            else:\n",
    "                gen = row[\"generation\"]\n",
    "\n",
    "            sex = constants.Sex.COMBINED if row[\"sex\"] == \"MF\" else row[\"sex\"]\n",
    "            animalgroup_data = {\n",
    "                \"experiment\": experiment_id,\n",
    "                \"name\": f\"{sex} {strain.name} {species.name}\",  # ex: female wistar rat\n",
    "                \"sex\": sex,\n",
    "                \"comments\": row[\"tg_comment\"] or \"\",\n",
    "                \"generation\": gen,\n",
    "                \"lifestage_at_assessment\": life_stage_map[row[\"life_stage\"]],\n",
    "                \"lifestage_at_exposure\": life_stage_map[row[\"life_stage\"]],\n",
    "                \"species_id\": species.id,\n",
    "                \"strain_id\": strain.id,\n",
    "            }\n",
    "            create_model_objects(animalgroup_url, [animalgroup_data])[0].json()\n",
    "\n",
    "print(toxref_to_hawc_tg_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create HAWC dosegroup, endpoint, dataextraction, and dose_response_group_level_data from ToxRef dose, dtg, dtg_effect, tg, tg_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get toxrefdb vocab terms\n",
    "toxrefdb_terms_df = pd.DataFrame(get_model_objects(toxrefdb_vocab_url).json())\n",
    "toxrefdb_terms_df = toxrefdb_terms_df.rename(\n",
    "    columns={\n",
    "        \"name_term_id\": \"name_term\",\n",
    "        \"system_term_id\": \"system_term\",\n",
    "        \"effect_term_id\": \"effect_term\",\n",
    "        \"effect_subtype_term_id\": \"effect_subtype_term\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary, unsure of the difference btw id and dose_id\n",
    "dose_group_id = 13000\n",
    "toxref_to_hawc_endpoint_map = {}\n",
    "endpoint_to_term_map = {}\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "\n",
    "for type in STUDY_TYPES:\n",
    "    cur.execute(query, (type,))\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Read into a df for data manipulation\n",
    "    column_names = [desc[0] for desc in cur.description]\n",
    "    df = pd.DataFrame(rows, columns=column_names)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # filter unique doses\n",
    "    unique_doses = df[\"dose_id\"].unique()\n",
    "    for dose_id in unique_doses:\n",
    "        dose_df = df[df[\"dose_id\"] == dose_id]\n",
    "\n",
    "        # format data\n",
    "        dose_df.loc[:, \"effect_var_type\"] = dose_df[\"effect_var_type\"].fillna(\n",
    "            constants.VarianceType.NA.label\n",
    "        )\n",
    "        dose_df.loc[:, \"dtg_effect_comment\"] = dose_df[\"dtg_effect_comment\"].fillna(\"\")\n",
    "        dose_df.loc[:, \"effect_val_unit\"] = dose_df[\"effect_val_unit\"].fillna(\n",
    "            \"\"\n",
    "        )  # set this to the default\n",
    "        dose_df[\"critical_effect\"] = (\n",
    "            dose_df[\"critical_effect\"]\n",
    "            .fillna(False)\n",
    "            .replace({\"True\": True, \"False\": False})\n",
    "            .astype(bool)\n",
    "        )\n",
    "\n",
    "        # filter unique dose treatment groups and effects for each dose\n",
    "        unique_dtg = dose_df[\"dtg_id\"].unique()\n",
    "        unique_tg_effect = dose_df[\"tg_effect_id\"].unique()\n",
    "\n",
    "        # create a hawc dosegroup for each toxrefdb dtg\n",
    "        for dtg_id in unique_dtg:\n",
    "            row = dose_df[dose_df[\"dtg_id\"] == dtg_id].iloc[0]\n",
    "\n",
    "            dose_adjusted = row[\"dose_adjusted\"]\n",
    "            if pd.isna(dose_adjusted):\n",
    "                dose_adjusted = row[\"conc\"] if not pd.isna(row[\"conc\"]) else 0\n",
    "                dose_unit, created = await sync_to_async(DoseUnits.objects.get_or_create)(\n",
    "                    name=\"ppm\"\n",
    "                )\n",
    "            else:\n",
    "                dose_unit = await sync_to_async(DoseUnits.objects.get)(name=\"mg/kg/d\")\n",
    "\n",
    "            dosegroup_data = {\n",
    "                \"dose\": dose_adjusted,\n",
    "                \"dose_group_id\": dose_group_id,  # TODO: id vs dose_group_id?\n",
    "                \"dose_units\": dose_unit,\n",
    "                \"treatment\": toxref_to_hawc_tg_map[row[\"tg_id\"]],\n",
    "            }\n",
    "            create_model_objects(dose_group_url, [dosegroup_data])[0].json()\n",
    "            # increment manual dose group id\n",
    "            dose_group_id += 1\n",
    "\n",
    "        # filter unique treatment group effects\n",
    "        for tg_effect_id in sorted(unique_tg_effect):\n",
    "            dtg_effect_df = dose_df[dose_df[\"tg_effect_id\"] == tg_effect_id]\n",
    "\n",
    "            for dtg_effect_id in dtg_effect_df[\"dtg_effect_id\"]:\n",
    "                row = dtg_effect_df[dtg_effect_df[\"dtg_effect_id\"] == dtg_effect_id].iloc[0]\n",
    "\n",
    "                # get endpoint mapping\n",
    "                endpoint_vocab = {\n",
    "                    \"name\": row[\"effect_desc\"],\n",
    "                    \"system\": row[\"endpoint_category\"],\n",
    "                    \"effect\": row[\"endpoint_type\"],\n",
    "                    \"effect_subtype\": row[\"endpoint_target\"],\n",
    "                }\n",
    "                # get term id info from existing toxrefdb df\n",
    "                condition = (\n",
    "                    toxrefdb_terms_df[list(endpoint_vocab)] == pd.Series(endpoint_vocab)\n",
    "                ).all(axis=1)\n",
    "                term_info = toxrefdb_terms_df[condition].iloc[0]\n",
    "                endpoint_data = {\"experiment\": study_to_exp_map[row[\"study_id\"]], **term_info}\n",
    "                if (\n",
    "                    f\"{row['endpoint_id']} {study_to_exp_map[row['study_id']]}\"\n",
    "                    not in toxref_to_hawc_endpoint_map.keys()\n",
    "                ):\n",
    "                    # create a new endpoint\n",
    "                    endpoint_id = (\n",
    "                        create_model_objects(endpoint_url, [endpoint_data])[0].json().get(\"id\")\n",
    "                    )\n",
    "                    toxref_to_hawc_endpoint_map[\n",
    "                        f\"{row['endpoint_id']} {study_to_exp_map[row['study_id']]}\"\n",
    "                    ] = endpoint_id\n",
    "                    endpoint_to_term_map[\n",
    "                        f\"{row['endpoint_id']} {study_to_exp_map[row['study_id']]}\"\n",
    "                    ] = term_info[\"name_term\"]\n",
    "                else:\n",
    "                    endpoint_id = toxref_to_hawc_endpoint_map[\n",
    "                        f\"{row['endpoint_id']} {study_to_exp_map[row['study_id']]}\"\n",
    "                    ]\n",
    "\n",
    "                # Create new observation time model if needed\n",
    "                if not pd.isna(row[\"time_unit\"]):\n",
    "                    obs_time_data = {\n",
    "                        \"observation_time\": row[\"time\"],\n",
    "                        \"observation_time_text\": row[\"time\"],\n",
    "                        \"observation_time_units\": observation_time_map[row[\"time_unit\"]],\n",
    "                        \"endpoint_id\": endpoint_id,\n",
    "                    }\n",
    "                    obs_time = create_model_objects(observation_time_url, [obs_time_data])[0].json()\n",
    "\n",
    "                    # if data extraction info is available, create model\n",
    "                    data_extraction_data = {\n",
    "                        \"experiment\": study_to_exp_map[row[\"study_id\"]],\n",
    "                        \"treatment\": toxref_to_hawc_tg_map[row[\"tg_id\"]],\n",
    "                        \"endpoint\": endpoint_id,\n",
    "                        \"dataset_type\": constants.DatasetType.NOT_REPORTED,  # no toxrefdb equivalent\n",
    "                        \"method_to_control_for_litter_effects\": constants.MethodToControlForLitterEffects.NR.label,  # no toxrefdb equivalent\n",
    "                        \"is_qualitative_only\": False\n",
    "                        if row[\"no_quant_data_reported\"] == \"f\"\n",
    "                        else True,\n",
    "                        \"variance_type\": variance_type_map[row[\"effect_var_type\"]],\n",
    "                        \"response_units\": str(row[\"effect_val_unit\"])[\n",
    "                            :32\n",
    "                        ],  # this is limited to 32 characters, unsure if we standardise somewhere\n",
    "                        \"dose_response_observations\": row[\"effect_comment\"] or \"NA\",\n",
    "                        \"observation_timepoint\": obs_time.get(\"id\"),\n",
    "                        \"result_details\": f\"{row['dtg_effect_comment']} {row['effect_comment']}\",\n",
    "                    }\n",
    "                    data_extraction = create_model_objects(\n",
    "                        data_extraction_url, [data_extraction_data]\n",
    "                    )[0].json()\n",
    "\n",
    "                    statistically_significant = (\n",
    "                        constants.StatisticallySignificant.YES\n",
    "                        if row[\"treatment_related\"] == \"t\"\n",
    "                        else constants.StatisticallySignificant.NO\n",
    "                    )\n",
    "\n",
    "                    # select appropriate dose column\n",
    "                    dose_col = \"dose_adjusted\"\n",
    "                    if pd.isna(dose_adjusted):\n",
    "                        dose_col = \"conc\"\n",
    "                    dose_adjusted = row[dose_col]\n",
    "\n",
    "                    # calculate NOEL: lowest dose with a critical effect\n",
    "                    filtered_df = dtg_effect_df[dtg_effect_df[\"critical_effect\"]]\n",
    "                    LOEL = filtered_df[dose_col].min()\n",
    "\n",
    "                    # calculate LOEL: highest dose with no critical effect\n",
    "                    filtered_df = dtg_effect_df[~dtg_effect_df[\"critical_effect\"]]\n",
    "                    NOEL = filtered_df[dose_col].max()\n",
    "\n",
    "                    effect_val = row[\"effect_val\"]\n",
    "                    if pd.isna(effect_val):\n",
    "                        effect_val = 0\n",
    "\n",
    "                    dose_response_group_level_data = {\n",
    "                        \"data_extraction\": data_extraction.get(\"id\"),\n",
    "                        \"treatment_name\": f\"{row['dose_duration']} {row['dose_duration_unit']} {row['admin_route']} {row['preferred_name']}\",  # from treatment model\n",
    "                        \"dose\": dose_adjusted,  # TODO: check this out in terms mapping w dtg\n",
    "                        \"n\": None if pd.isna(row[\"n\"]) else row[\"n\"],\n",
    "                        \"response\": effect_val,  # TODO verify mapping and default\n",
    "                        \"variance\": None if pd.isna(row[\"effect_var\"]) else row[\"effect_var\"],\n",
    "                        \"treatment_related_effect\": treatment_related_map[\n",
    "                            str(row[\"treatment_related\"]).lower()\n",
    "                        ],\n",
    "                        \"statistically_significant\": statistically_significant,\n",
    "                        \"p_value\": 0.05\n",
    "                        if row[\"treatment_related\"] == \"t\"\n",
    "                        else \"\",  # TODO: ask about default p_value\n",
    "                        \"NOEL\": round(NOEL)\n",
    "                        if not pd.isna(NOEL)\n",
    "                        else -999,  # HAWC requires ints not floats\n",
    "                        \"LOEL\": round(LOEL)\n",
    "                        if not pd.isna(LOEL)\n",
    "                        else -999,  # HAWC requires ints not floats\n",
    "                    }\n",
    "                    create_model_objects(\n",
    "                        dose_response_group_level_data_url, [dose_response_group_level_data]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrate observation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_query = (\n",
    "    \"\"\"SELECT study_id, endpoint_id, tested_status, reported_status FROM prod_toxrefdb_2_1.obs\"\"\"\n",
    ")\n",
    "\n",
    "obs_data = fetch_data_from_source(obs_query, source_conn)\n",
    "# only migrate toxrefdb observations linked to an existing toxrefdb endpoint effect\n",
    "obs_mapping = []\n",
    "\n",
    "for row in obs_data:\n",
    "    if f\"{float(row[1])} {study_to_exp_map[row[0]]}\" in endpoint_to_term_map.keys():\n",
    "        obs_mapping.append(\n",
    "            {\n",
    "                \"experiment_id\": study_to_exp_map[row[0]],\n",
    "                # key: endpoint_id and experiment_id\n",
    "                \"endpoint_id\": endpoint_to_term_map[f\"{float(row[1])} {study_to_exp_map[row[0]]}\"],\n",
    "                \"tested_status\": row[2],\n",
    "                \"reported_status\": row[3],\n",
    "            }\n",
    "        )\n",
    "\n",
    "for row in obs_mapping:\n",
    "    print(\"row; \", row)\n",
    "    object, created = await sync_to_async(models.Observation.objects.get_or_create)(**row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the cursor and connection\n",
    "# cur.close()\n",
    "# source_conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
